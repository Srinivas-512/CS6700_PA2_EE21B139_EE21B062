{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "--pnvZ-P9Bd6",
        "AvoWSnOE9LG9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "--pnvZ-P9Bd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import gym\n",
        "from scipy.special import softmax\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from collections import deque, namedtuple\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "LghyPWTm5cie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environments"
      ],
      "metadata": {
        "id": "AvoWSnOE9LG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cartpole"
      ],
      "metadata": {
        "id": "-FinOrtx-50d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_cartpole = gym.make(\"CartPole-v1\")"
      ],
      "metadata": {
        "id": "dJHgULRb9NUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Acrobot"
      ],
      "metadata": {
        "id": "_LrT1N32_Eoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_acrobot = gym.make(\"Acrobot-v1\")"
      ],
      "metadata": {
        "id": "xFYbeLPL9NXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions (run after Dueling DQN classes, before environment grid search and inference)"
      ],
      "metadata": {
        "id": "iY9U2wWrFW6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid Search function that takes in a set of parameters to search over and finds the best set by minimizing average regret over 5 seeds for each set of parameters. **These helper functions should be run after the DDQN classes have been run**"
      ],
      "metadata": {
        "id": "OVoBxeCLFfA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame({\"buffer_size\" : [0], \"batch_size\": [0], \"gamma\": [0], \"lr\": [0], \"update_every\": [0], \"regret\":[0]})"
      ],
      "metadata": {
        "id": "QzYfVPQTGFEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search(env, env_vars, general_vars, policy, update_type):\n",
        "\n",
        "  var_lists = env_vars.values()\n",
        "  gen_var_lists = general_vars[policy].values()\n",
        "\n",
        "  env_grid = list(itertools.product(*var_lists))\n",
        "  gen_grid = list(itertools.product(*gen_var_lists))\n",
        "  grid = [[*x, *y] for x in env_grid for y in gen_grid]\n",
        "  grid_search_results = []\n",
        "\n",
        "  best_regret = 1e9\n",
        "\n",
        "  state_shape = env.observation_space.shape[0]\n",
        "  action_shape = env.action_space.n\n",
        "\n",
        "  i=0\n",
        "  seeds = [0, 1, 2, 3, 4]\n",
        "\n",
        "  if(policy==\"Epsilon Greedy\"):\n",
        "\n",
        "    for combination in grid:\n",
        "      i+=1\n",
        "      buffer_size, batch_size, gamma, lr, update_every, start, end, decay = combination\n",
        "      print(f\"combination: {combination} , number: {i}\")\n",
        "      regret = 0\n",
        "\n",
        "      for seed in seeds:\n",
        "        agent = Agent(state_shape, action_shape, seed, update_type, policy, buffer_size,\n",
        "                              lr, gamma, update_every, batch_size)\n",
        "        _, scores, regrets = ddqn(env, 500, 500, start, end, decay, 1, 1, 1, agent)\n",
        "        regret += np.mean(regrets)\n",
        "\n",
        "      regret/=5\n",
        "\n",
        "      if regret < best_regret:\n",
        "        best_regret = regret\n",
        "        grid_search_results = [buffer_size, batch_size, gamma, lr, update_every, start, end, decay]\n",
        "\n",
        "      results_df.loc[len(results_df)] = [buffer_size, batch_size, gamma, lr, update_every, regret]\n",
        "\n",
        "      print(f\"Regret - {regret}\")\n",
        "\n",
        "  elif(policy==\"Softmax\"):\n",
        "\n",
        "    for combination in grid:\n",
        "      i+=1\n",
        "      buffer_size, batch_size, gamma, lr, update_every, start, end, decay = combination\n",
        "      print(f\"combination: {combination} , number: {i}\")\n",
        "      regret = 0\n",
        "\n",
        "      for seed in seeds:\n",
        "        agent = Agent(state_shape, action_shape, seed, update_type, policy, buffer_size,\n",
        "                              lr, gamma, update_every, batch_size)\n",
        "        _, scores, regrets = ddqn(env, 500, 500, 1, 1, 1, start, end, decay, agent)\n",
        "        regret += np.mean(regrets)\n",
        "\n",
        "      regret/=5\n",
        "\n",
        "      if regret < best_regret:\n",
        "        best_regret = regret\n",
        "        grid_search_results = [buffer_size, batch_size, gamma, lr, update_every, start, end, decay]\n",
        "\n",
        "      results_df.loc[len(results_df)] = [buffer_size, batch_size, gamma, lr, update_every, regret]\n",
        "\n",
        "      print(f\"Regret - {regret}\")\n",
        "\n",
        "  return grid_search_results, best_regret"
      ],
      "metadata": {
        "id": "3n_VJljpFZ3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we have a plot function that plots the regrets, scores and running scores for each type of update rule over 5 seeds. This function also computes the mean and std over the seeds before plotting."
      ],
      "metadata": {
        "id": "5qbsnigRFxtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(regrets_list_type1, scores_list_type1, regrets_list_type2, scores_list_type2, running_scores_1, running_scores_2, reward_threshold): #extended_rewards,\n",
        "\n",
        "  max_eps = max(len(lst) for lst in regrets_list_type1)\n",
        "  extended_regrets_type1 = [lst + [0] * (max_eps - len(lst)) for lst in regrets_list_type1]\n",
        "  extended_scores_type1 = [lst + [0] * (max_eps - len(lst)) for lst in scores_list_type1]\n",
        "  extended_regrets_type2 = [lst + [0] * (max_eps - len(lst)) for lst in regrets_list_type2]\n",
        "  extended_scores_type2 = [lst + [0] * (max_eps - len(lst)) for lst in scores_list_type2]\n",
        "  r1 = np.empty((max_eps, 5))\n",
        "  s1 = np.empty((max_eps, 5))\n",
        "  r2 = np.empty((max_eps, 5))\n",
        "  s2 = np.empty((max_eps, 5))\n",
        "  rs1 = np.empty((max_eps, 5))\n",
        "  rs2 = np.empty((max_eps, 5))\n",
        "\n",
        "  for i in range(5):\n",
        "    for j in range(max_eps):\n",
        "      r1[j][i] = extended_regrets_type1[i][j]\n",
        "      s1[j][i] = extended_scores_type1[i][j]\n",
        "      r2[j][i] = extended_regrets_type2[i][j]\n",
        "      s2[j][i] = extended_scores_type2[i][j]\n",
        "      rs1[j][i] = running_scores_1[i][j]\n",
        "      rs2[j][i] = running_scores_2[i][j]\n",
        "\n",
        "  r1_mean_values = np.mean(r1, axis=1)\n",
        "  r1_std_dev = np.std(r1, axis=1)\n",
        "  s1_mean_values = np.mean(s1, axis=1)\n",
        "  s1_std_dev = np.std(s1, axis=1)\n",
        "  r2_mean_values = np.mean(r2, axis=1)\n",
        "  r2_std_dev = np.std(r2, axis=1)\n",
        "  s2_mean_values = np.mean(s2, axis=1)\n",
        "  s2_std_dev = np.std(s2, axis=1)\n",
        "  rs1_mean_values = np.mean(rs1, axis=1)\n",
        "  rs1_std_dev = np.std(rs1, axis=1)\n",
        "  rs2_mean_values = np.mean(rs2, axis=1)\n",
        "  rs2_std_dev = np.std(rs2, axis=1)\n",
        "\n",
        "  x = np.arange(max_eps)\n",
        "  t = [reward_threshold]*max_eps\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(x, r1_mean_values, color='blue', linestyle='-', label='Type 1 Update')\n",
        "  plt.fill_between(x, r1_mean_values - r1_std_dev, r1_mean_values + r1_std_dev, color='lightblue')\n",
        "  plt.plot(x, r2_mean_values, color='red', linestyle='-', label='Type 2 Update')\n",
        "  plt.fill_between(x, r2_mean_values - r2_std_dev, r2_mean_values + r2_std_dev, color='mistyrose')\n",
        "  plt.plot(x, t, color='green', linestyle='-', label='Threshold')\n",
        "  plt.xlabel('Episode')\n",
        "  plt.ylabel('Regret')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(x, s1_mean_values, color='blue', linestyle='-', label='Type 1 Update')\n",
        "  plt.fill_between(x, s1_mean_values - s1_std_dev, s1_mean_values + s1_std_dev, color='lightblue')\n",
        "  plt.plot(x, s2_mean_values, color='red', linestyle='-', label='Type 2 Update')\n",
        "  plt.fill_between(x, s2_mean_values - s2_std_dev, s2_mean_values + s2_std_dev, color='mistyrose')\n",
        "  plt.plot(x, t, color='green', linestyle='-', label='Threshold')\n",
        "  plt.xlabel('Episode')\n",
        "  plt.ylabel('Reward')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(x, rs1_mean_values, color='blue', linestyle='-', label='Type 1 Update')\n",
        "  plt.fill_between(x, rs1_mean_values - rs1_std_dev, rs1_mean_values + rs1_std_dev, color='lightblue')\n",
        "  plt.plot(x, rs2_mean_values, color='red', linestyle='-', label='Type 2 Update')\n",
        "  plt.fill_between(x, rs2_mean_values - rs2_std_dev, rs2_mean_values + rs2_std_dev, color='mistyrose')\n",
        "  plt.plot(x, t, color='green', linestyle='-', label='Threshold')\n",
        "  plt.xlabel('Episode')\n",
        "  plt.ylabel('Running Reward')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "zAVl1VMoF-PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we have an inference function that takes in the optimal set of hyperparameters and runs inference on each update type (taken in as an argument) in each environment (also an argument)."
      ],
      "metadata": {
        "id": "noRQjRedGJRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(env, vars, policy, update_type):\n",
        "\n",
        "  state_shape = env.observation_space.shape[0]\n",
        "  action_shape = env.action_space.n\n",
        "\n",
        "  seeds = [0, 1, 2, 3, 4]\n",
        "  regrets_list = []\n",
        "  scores_list = []\n",
        "  running_scores_list = []\n",
        "\n",
        "  buffer_size, batch_size, gamma, lr, update_every, start, end, decay = vars\n",
        "\n",
        "  if(policy==\"Epsilon Greedy\"):\n",
        "\n",
        "    for seed in seeds:\n",
        "      print(seed)\n",
        "      agent = Agent(state_shape, action_shape, seed, update_type, policy, buffer_size,\n",
        "                            lr, gamma, update_every, batch_size)\n",
        "      _, scores, regrets, running_scores = ddqn(env, 500, 500, start, end, decay, 1, 1, 1, agent)\n",
        "      regrets_list.append(regrets)\n",
        "      scores_list.append(scores)\n",
        "      running_scores_list.append(running_scores)\n",
        "\n",
        "  elif(policy==\"Softmax\"):\n",
        "\n",
        "    for seed in seeds:\n",
        "      print(seed)\n",
        "      agent = Agent(state_shape, action_shape, seed, update_type, policy, buffer_size,\n",
        "                            lr, gamma, update_every, batch_size)\n",
        "      _, scores, regrets, running_scores = ddqn(env, 500, 500, 1, 1, 1, start, end, decay, agent)\n",
        "      regrets_list.append(regrets)\n",
        "      scores_list.append(scores)\n",
        "      running_scores_list.append(running_scores)\n",
        "\n",
        "  return regrets_list, scores_list, running_scores_list"
      ],
      "metadata": {
        "id": "u8KTSsJaGJuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dueling DQN"
      ],
      "metadata": {
        "id": "1_Rratx18_iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "jMhnFL1UYuFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the general policy variables, start, end and decay values"
      ],
      "metadata": {
        "id": "SKWHSB9uHjyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "general_vars = {\n",
        "    \"Epsilon Greedy\": {\n",
        "        \"eps_start\" : [0.1],\n",
        "        \"eps_end\" : {0.0001},\n",
        "        \"eps_decay\" : {0.995}\n",
        "    },\n",
        "    \"Softmax\":{\n",
        "        \"temp_start\" : [1],\n",
        "        \"temp_end\" : {0.01},\n",
        "        \"temp_decay\" : {0.995}\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "RZ3CItKynwZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two update types are defined below as mentioned in the problem statement:"
      ],
      "metadata": {
        "id": "Nb8qWid7Hou2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def type1Update(advantages, values):\n",
        "    return values + (advantages - advantages.mean())\n",
        "\n",
        "def type2Update(advantages, values):\n",
        "  return values + (advantages - torch.max(advantages))"
      ],
      "metadata": {
        "id": "V81MWHakcEr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Q Network is defined below:"
      ],
      "metadata": {
        "id": "l8WMLFPhHvQJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfjNyBJseN8l"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, seed):\n",
        "    super(QNetwork, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "    self.feauture_layer = nn.Sequential(\n",
        "        nn.Linear(self.input_dim, 64),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.value_stream = nn.Sequential(\n",
        "        nn.Linear(64, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 1)\n",
        "    )\n",
        "\n",
        "    self.advantage_stream = nn.Sequential(\n",
        "        nn.Linear(64, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, self.output_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, state, update_type):\n",
        "      features = self.feauture_layer(state)\n",
        "      values = self.value_stream(features)\n",
        "      advantages = self.advantage_stream(features)\n",
        "      qvals = update_type(advantages, values)\n",
        "      return qvals"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we have the replay buffer"
      ],
      "metadata": {
        "id": "3e7lcOVeHyhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer():\n",
        "  def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "    self.action_size = action_size\n",
        "    self.memory = deque(maxlen=buffer_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "    self.seed = random.seed(seed)\n",
        "\n",
        "  def add(self, state, action, reward, next_state, done):\n",
        "    e = self.experience(state, action, reward, next_state, done)\n",
        "    self.memory.append(e)\n",
        "\n",
        "  def sample(self):\n",
        "    experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "    states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "    actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "    rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "    next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "    dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "    return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.memory)"
      ],
      "metadata": {
        "id": "zwNs6-t89JiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The agent class is defined as follows:"
      ],
      "metadata": {
        "id": "n37f97vnH1FI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "  def __init__(self, state_size, action_size, seed, update_type, policy_type, buffer_size, lr, gamma, update_every, batch_size):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.seed = random.seed(seed)\n",
        "    self.update_type = update_type\n",
        "    self.policy_type = policy_type\n",
        "    self.buffer_size = buffer_size\n",
        "    self.batch_size = batch_size\n",
        "    self.update_every = update_every\n",
        "    self.lr = lr\n",
        "    self.gamma = gamma\n",
        "    self.seed=seed\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
        "    torch.manual_seed(seed)\n",
        "    self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
        "    self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=self.lr)\n",
        "\n",
        "    self.memory = ReplayBuffer(action_size, self.buffer_size, self.batch_size, seed)\n",
        "\n",
        "    self.t_step = 0\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "    if len(self.memory) >= self.batch_size:\n",
        "        experiences = self.memory.sample()\n",
        "        self.learn(experiences, self.gamma)\n",
        "\n",
        "    self.t_step = (self.t_step + 1) % self.update_every\n",
        "    if self.t_step == 0:\n",
        "\n",
        "        self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "  def act(self, state, eps=0., temp=1.):\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "    self.qnetwork_local.eval()\n",
        "    with torch.no_grad():\n",
        "        action_values = self.qnetwork_local(state, self.update_type)\n",
        "    self.qnetwork_local.train()\n",
        "\n",
        "    if(self.policy_type == \"Epsilon Greedy\"):\n",
        "      if random.random() > eps:\n",
        "          return np.argmax(action_values.cpu().data.numpy())\n",
        "      else:\n",
        "          return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    else:\n",
        "      action_values = action_values.cpu().data.numpy().reshape(action_values.shape[1]) / temp\n",
        "      probabilities = softmax(action_values)\n",
        "      return np.random.choice(len(probabilities), p=probabilities)\n",
        "\n",
        "  def learn(self, experiences, gamma):\n",
        "    states, actions, rewards, next_states, dones = experiences\n",
        "    Q_targets_next = self.qnetwork_target(next_states, self.update_type).detach().max(1)[0].unsqueeze(1)\n",
        "    Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "    Q_expected = self.qnetwork_local(states, self.update_type).gather(1, actions)\n",
        "\n",
        "    loss = F.mse_loss(Q_expected, Q_targets)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()"
      ],
      "metadata": {
        "id": "pI9r9DjJXEtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dueling dqn algorithm is executed using the function below:"
      ],
      "metadata": {
        "id": "jgKHEIBHITFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ddqn(env, n_episodes, max_t, eps_start, eps_end, eps_decay, temp_start, temp_end, temp_decay, agent):\n",
        "\n",
        "    scores_window = deque(maxlen=100)\n",
        "    eps = eps_start\n",
        "    temp = temp_start\n",
        "\n",
        "    scores, regrets = [], []\n",
        "    running_avg_scores = []\n",
        "\n",
        "    running_reward = 10 # change to -500 for acrobot\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps, temp)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores_window.append(score)\n",
        "        scores.append(score)\n",
        "        regrets.append(500-score) # change to regrets.append(-score) for acrobot\n",
        "        running_reward = 0.05 * score + (1 - 0.05) * running_reward\n",
        "        running_avg_scores.append(running_reward)\n",
        "\n",
        "        eps = max(eps_end, eps_decay*eps)\n",
        "        temp = max(temp_end, temp_decay*temp)\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tRegret: {:.2f}'.format(i_episode, np.mean(scores_window), regrets[-1]), end=\"\")\n",
        "\n",
        "        if i_episode % 100 == 0:\n",
        "           print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "    return True, scores, regrets, running_avg_scores"
      ],
      "metadata": {
        "id": "_sUbI8lWY2uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cartpole Training (Grid Search and Inference)"
      ],
      "metadata": {
        "id": "-ipJ0PwoHSpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cartpole_vars = {\n",
        "    \"BUFFER_SIZE\" : [int(100e3), int(250e3), int(500e3)],  # replay buffer size\n",
        "    \"BATCH_SIZE\" : [32, 64, 128],                          # minibatch size\n",
        "    \"GAMMA\" : [0.99],                                      # discount factor\n",
        "    \"LR\" : [1e-3, 1e-4, 1e-5],                             # learning rate\n",
        "    \"UPDATE_EVERY\" : [10, 20, 50]                          # how often to update the network (When Q target is present)\n",
        "}"
      ],
      "metadata": {
        "id": "Gvewvx2QHDKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = env_cartpole\n",
        "\n",
        "# grid_search_results_type1, best_regret = grid_search(env, cartpole_vars, general_vars, \"Softmax\", type1Update)\n",
        "# grid_search_results_type2, best_regret = grid_search(env, cartpole_vars, general_vars, \"Softmax\", type2Update)\n",
        "\n",
        "threshold = 195\n",
        "cartpole_vars_optimal = [int(500e3), 64, 0.99, 1e-3, 20, 1, 1, 0]\n",
        "regrets_type1, scores_type1, running_scores_type1 = inference(env, cartpole_vars_optimal, \"Softmax\", type1Update)\n",
        "regrets_type2, scores_type2, running_scores_type2 = inference(env, cartpole_vars_optimal, \"Softmax\", type2Update)"
      ],
      "metadata": {
        "id": "zb18kU0NgIwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(regrets_type1, scores_type1, regrets_type2, scores_type2, running_scores_type1, running_scores_type2, threshold)"
      ],
      "metadata": {
        "id": "L9gjAVqSHAH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Acrobot Training (Grid Search and Inference)"
      ],
      "metadata": {
        "id": "WqdIcsUCHXgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acrobot_vars = {\n",
        "    \"BUFFER_SIZE\" : [int(100e3), int(250e3), int(500e3)],  # replay buffer size\n",
        "    \"BATCH_SIZE\" : [32, 64, 128],                          # minibatch size\n",
        "    \"GAMMA\" : [0.99],                                      # discount factor\n",
        "    \"LR\" : [1e-3, 1e-4, 1e-5],                             # learning rate\n",
        "    \"UPDATE_EVERY\" : [10, 20, 50]                          # how often to update the network (When Q target is present)\n",
        "}"
      ],
      "metadata": {
        "id": "ok3gX28JHH3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = env_acrobot\n",
        "\n",
        "# grid_search_results_type1, best_regret = grid_search(env, acrobot_vars, general_vars, \"Softmax\", type1Update)\n",
        "# grid_search_results_type2, best_regret = grid_search(env, acrobot_vars, general_vars, \"Softmax\", type2Update)\n",
        "\n",
        "threshold = -100\n",
        "acrobot_vars_optimal = [int(500e3), 64, 0.99, 1e-3, 20, 0.01, 0.0001, 0.99]\n",
        "regrets_type1, scores_type1, running_scores_type1 = inference(env, acrobot_vars_optimal, \"Epsilon Greedy\", type1Update)\n",
        "regrets_type2, scores_type2, running_scores_type2 = inference(env, acrobot_vars_optimal, \"Epsilon Greedy\", type2Update)"
      ],
      "metadata": {
        "id": "XOuIrbyp5N84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(regrets_type1, scores_type1, regrets_type2, scores_type2, running_scores_type1, running_scores_type2, threshold)"
      ],
      "metadata": {
        "id": "WwfInJ5IFAu5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}